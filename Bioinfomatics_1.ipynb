{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWFIdlE8mCSi"
   },
   "source": [
    "\n",
    "**Title: Prokaryotic and eukaryotic promoters Recognition and Prediction**\n",
    "\n",
    "Environment Requirement: Due to the poor performance of t-SNE during visualization on CPU runtime (taking an average of 16 minutes to generate a 3x3 grid view scatter plot when k=4), please switch to GPU runtime before running this notebook. This will significantly improve processing speed and efficiency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KunlB9DmCwn"
   },
   "source": [
    "#2. Loading the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Y_EpS09whKJ"
   },
   "source": [
    "We will employ the Python [requests](https://pypi.org/project/requests/)\n",
    "library and SeqIO from [Biopython](https://biopython.org/) to parse FASTA files efficiently.\n",
    "\n",
    "**Bio.SeqIO.parse(fasta_io, \"fasta\") → a function from the Biopython library that parses biological sequence files. It extracts sequences while ignoring header lines (>identifier).\n",
    "Handles multi-line sequences automatically, reconstructing them into a single string.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-JxXBzudwMjJ",
    "outputId": "02e7da0f-cfff-4721-9c78-e46a4788f3d5"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "!pip install biopython\n",
    "from Bio import SeqIO\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbI2iy83xggC"
   },
   "source": [
    "This function fetches and parses a FASTA file from GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "j7iSE5pvv-VZ"
   },
   "outputs": [],
   "source": [
    "# Function to load and parse FASTA sequences\n",
    "def load_fasta(filename):\n",
    "\n",
    "    url = BASE_URL + filename  # Construct full URL\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Ensure successful download\n",
    "\n",
    "    sequences = []\n",
    "    fasta_io = io.StringIO(response.text)\n",
    "\n",
    "    for record in SeqIO.parse(fasta_io, \"fasta\"):\n",
    "        sequences.append((str(record.seq).upper(), datasets[filename]))  # Assign correct label\n",
    "\n",
    "    return sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmO_Rhen0l-9"
   },
   "source": [
    "This step downloads and parses FASTA files from GitHub, extracts nucleotide sequences, assigns labels (1 for promoters, 0 for non-promoters), and structures the data for two experiments:\n",
    "\n",
    "-Experiment 1:\n",
    "\n",
    "positive = ‘Arabidopsis_tata.fa’, negative = ‘Arabidopsis_non_prom_big.fa’.\n",
    "\n",
    "-Experiment 2: Non-TATA promoters vs. non-promoters\n",
    "\n",
    "positive = ‘Arabidopsis_non_tata.fa’, negative = ‘Arabidopsis_non_prom_big.fa’.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Tfz8ilXBeXz",
    "outputId": "2dee5621-1f93-4342-829c-05508171a847"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Base URL for the dataset\n",
    "BASE_URL = \"https://raw.githubusercontent.com/solovictor/CNNPromoterData/refs/heads/master/\"\n",
    "\n",
    "# Filenames and corresponding labels\n",
    "datasets = {\n",
    "    \"Arabidopsis_tata.fa\": 1,        # Positive (TATA)\n",
    "    \"Arabidopsis_non_tata.fa\": 1,    # Positive (Non-TATA)\n",
    "    \"Arabidopsis_non_prom_big.fa\": 0 # Negative\n",
    "}\n",
    "\n",
    "# Load datasets\n",
    "positive_tata = load_fasta(\"Arabidopsis_tata.fa\")\n",
    "positive_non_tata = load_fasta(\"Arabidopsis_non_tata.fa\")\n",
    "negative = load_fasta(\"Arabidopsis_non_prom_big.fa\")\n",
    "\n",
    "# Create experiments\n",
    "experiment_1 = positive_tata + negative\n",
    "experiment_2 = positive_non_tata + negative\n",
    "\n",
    "# Display dataset sizes\n",
    "print(f\"Experiment 1: {len(experiment_1)} sequences\")\n",
    "print(f\"Experiment 2: {len(experiment_2)} sequences\")\n",
    "\n",
    "# Example sequence preview\n",
    "print(\"Sample sequence:\", experiment_1[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbJEapgFvwOQ"
   },
   "source": [
    "#3. Data Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHaBd4HryMAm"
   },
   "source": [
    "We will use [itertools](https://docs.python.org/3/library/itertools.html) library to create all nucleotide combinations and BioPython's Seq object provides biological sequence handling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9WjKkmQGyMOz"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from Bio.Seq import Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8OTy0ROzAGb"
   },
   "source": [
    "This function moves one nucleotide at a time, capturing overlapping k-mers as step size = 1 (by default)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lngSBD_azKNI"
   },
   "outputs": [],
   "source": [
    "# Function to generate k-mers using BioPython\n",
    "def generate_kmers(sequence, k):\n",
    "    return [str(Seq(sequence[i:i+k])) for i in range(len(sequence) - k + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PS4j8pX5zQQI"
   },
   "source": [
    "And generate a normalized k-mer frequency vector for a given sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "WyoOu3uvzQdQ"
   },
   "outputs": [],
   "source": [
    "# Function to compute k-mer frequency vector\n",
    "def kmer_frequency_vector(sequence, k):\n",
    "\n",
    "    kmer_list = [''.join(p) for p in itertools.product(\"ACGT\", repeat=k)]  # Generate all possible k-mers\n",
    "    kmer_counts = Counter(generate_kmers(sequence, k))  # Count occurrences using BioPython sequences\n",
    "\n",
    "    # Normalize frequencies\n",
    "    total_kmers = sum(kmer_counts.values())\n",
    "    kmer_freq_vector = {kmer: kmer_counts.get(kmer, 0) / total_kmers for kmer in kmer_list}\n",
    "\n",
    "    return np.array(list(kmer_freq_vector.values()))  # Convert to numerical format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XysbNLKczXjb"
   },
   "source": [
    "Then encodes all sequences in a dataset using k-mer frequency vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dyuFnPI_yT1F"
   },
   "outputs": [],
   "source": [
    "# Function to encode datasets\n",
    "def encode_dataset(dataset, k):\n",
    "\n",
    "    encoded_data = np.array([kmer_frequency_vector(seq, k) for seq, _ in dataset])\n",
    "    labels = np.array([label for _, label in dataset])  # Extract labels\n",
    "    return encoded_data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gGgyduW81Fx"
   },
   "source": [
    "This Step extracts overlapping k-mers (subsequences) using BioPython. Counts and normalizes k-mer occurrences for valid nucleotides (A, C, G, T). Then generates numerical feature vectors for experiment_1 & experiment_2 datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C8wAtJkTvwYx",
    "outputId": "b472fb43-5390-4693-dd75-8726358a8a1a"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define k values (4 to 6) for bioinformatics analysis, this is Global parameter\n",
    "k_values = [4, 5, 6]\n",
    "\n",
    "# Encode Experiment 1 & 2 datasets for multiple k values\n",
    "encoded_experiments = {k: {\n",
    "    \"experiment_1\": encode_dataset(experiment_1, k),\n",
    "    \"experiment_2\": encode_dataset(experiment_2, k)\n",
    "} for k in k_values}\n",
    "\n",
    "# Display dataset dimensions\n",
    "for k, data in encoded_experiments.items():\n",
    "    print(f\"\\nEncoding for k={k}:\")\n",
    "    print(f\"  Experiment 1: {data['experiment_1'][0].shape}, Labels: {data['experiment_1'][1].shape}\")\n",
    "    print(f\"  Experiment 2: {data['experiment_2'][0].shape}, Labels: {data['experiment_2'][1].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5PayANOFaly"
   },
   "source": [
    "**Consideratios*\n",
    "\n",
    "e.g. Encoding for k=4:\n",
    "  Experiment 1: (12956, 256), Labels: (12956,)\n",
    "\n",
    "Meaning 12956 sample count and 256 Features.\n",
    "\n",
    "t-SNE is not designed for very large datasets and can become computationally expensive for more than 10,000 samples or dimensions are above 50. Need to apply further dimension reduction like PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0Er8zJc9xxI"
   },
   "source": [
    "#4. Understanding your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uz8ACYA19yF6"
   },
   "source": [
    "##4.1 Class Distribution\n",
    "Assess the distribution of instances within each class, distinguishing between positive and negative examples. Determine whether the datasets for these experiments are balanced or exhibit class imbalance.\n",
    "\n",
    "The ratios suggested in this Google ML Crash Course are typical.\n",
    "\n",
    "*   60% (majority class)/40% (minority class) split would not be considered problematic\n",
    "*   90% (majority class)/10% (minority class) split would be high\n",
    "*   Between normal and high would be considered mild\n",
    "*   99%/1% or worse would be extreme\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EvaPjGEL9yRb",
    "outputId": "005efc14-3607-4f1e-c284-5558b4354508"
   },
   "outputs": [],
   "source": [
    "print(\"Type of experiment_1: \", type(experiment_1)) #take a peek at experiment_1 data type\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Convert datasets to DataFrames\n",
    "df_exp1 = pd.DataFrame(experiment_1, columns=[\"Sequence\", \"Label\"])\n",
    "df_exp2 = pd.DataFrame(experiment_2, columns=[\"Sequence\", \"Label\"])\n",
    "\n",
    "# Compute class distribution in percentage\n",
    "exp1_distribution = df_exp1[\"Label\"].value_counts(normalize=True) * 100\n",
    "exp2_distribution = df_exp2[\"Label\"].value_counts(normalize=True) * 100\n",
    "\n",
    "#Class distribution summary\n",
    "exp1_summary = f\"Experiment 1 Class Distribution:\\nPositive: {exp1_distribution[1]:.2f}%\\nNegative: {exp1_distribution[0]:.2f}%\\n\"\n",
    "exp2_summary = f\"Experiment 2 Class Distribution:\\nPositive: {exp2_distribution[1]:.2f}%\\nNegative: {exp2_distribution[0]:.2f}%\\n\"\n",
    "\n",
    "# Print formatted results\n",
    "print(exp1_summary)\n",
    "print(exp2_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtWp1AgpC7LP"
   },
   "source": [
    "##4.2 Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cs86mk_Z0F6w"
   },
   "source": [
    "In this step we will\n",
    "\n",
    "*   Generate a graph representing the positive examples.\n",
    "*   Generate a graph representing the negative examples.\n",
    "*   Generate a composite graph that includes both positive and negative examples\n",
    "\n",
    "It is advisable to investigate the influence of various parameters. For instance, consider altering the value of\n",
    " between 4 and 6. Additionally, parameters such as ‘perplexity’ and ‘early_exaggeration’ are critical and warrant exploration due to their potential impact on the results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tX2-rcHrLOQd"
   },
   "source": [
    "Check  CUDA Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zDPJS5-KLNmQ",
    "outputId": "ebca960a-d220-4384-b316-c0f025a63b00"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Check if CUDA is enabled\n",
    "print(torch.version.cuda)  # Display the CUDA version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4L3PgIFH0gFa"
   },
   "source": [
    "Necessary pip installation commands for running it on NVIDIA GPUs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fyso6cbN0g-N",
    "outputId": "f05604dd-fd35-4bd8-c328-051add452929"
   },
   "outputs": [],
   "source": [
    "!pip install cuml-cu12 --extra-index-url=https://pypi.nvidia.com\n",
    "!pip install cupy-cuda12x\n",
    "!pip uninstall -y numpy\n",
    "!pip install --upgrade \"numpy<2.0.0\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDjE_72A1nB_"
   },
   "source": [
    "We will use [cuml](https://github.com/rapidsai/cuml) for GPU-accelerated t-SNE. And [cupy](https://github.com/cupy/cupy) to handle GPU-based arrays (cp.ndarray) instead of CPU-based NumPy arrays (np.ndarray), which converts PCA-reduced NumPy data → CuPy arrays before passing to TSNE.\n",
    "\n",
    "**If you encouter ImportError: cannot import name 'intp' from 'numpy._core' (/usr/local/lib/python3.11/dist-packages/numpy/_core/__init__.py), Downgrade NumPy to a Compatible Version*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "AsRiO-2K1nO8"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "from cuml.manifold import TSNE  # GPU-accelerated t-SNE\n",
    "from sklearn.decomposition import PCA\n",
    "import cupy as cp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKTLbAlD3Uww"
   },
   "source": [
    "This fucntion projects high-dimensional k-mer encoded data into 2D space using PCA + GPU t-SNE and plots results in a grid.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "TgS1TC6-ZIaA"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create a directory to save plots\n",
    "output_dir = \"tsne_plots\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "v8bKJUNn3U48"
   },
   "outputs": [],
   "source": [
    "# Function to perform GPU-accelerated t-SNE visualization\n",
    "def plot_tsne_grid(encoded_data, labels, title, perplexities, exaggerations, pca_components=30):\n",
    "\n",
    "    num_rows = len(perplexities) * len(exaggerations)  # Each perplexity & exaggeration pair gets its own row\n",
    "    num_cols = 3  # Columns represent Positive, Negative, and Combined\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(6 * num_cols, 6 * num_rows))  # Increase figure size for better visibility\n",
    "\n",
    "    row_index = 0\n",
    "    for perplexity in perplexities:\n",
    "        for early_exaggeration in exaggerations:\n",
    "\n",
    "            # Apply PCA for dimensionality reduction if needed\n",
    "            if encoded_data.shape[1] > pca_components:\n",
    "                pca = PCA(n_components=pca_components, random_state=42)\n",
    "                reduced_data = pca.fit_transform(encoded_data)\n",
    "            else:\n",
    "                reduced_data = encoded_data\n",
    "\n",
    "            # Convert data to GPU format using CuPy\n",
    "            reduced_data_gpu = cp.asarray(reduced_data)\n",
    "\n",
    "            # Perform optimized t-SNE projection using cuML\n",
    "            tsne = TSNE(n_components=2, perplexity=perplexity, early_exaggeration=early_exaggeration, random_state=42)\n",
    "            transformed_data = tsne.fit_transform(reduced_data_gpu)\n",
    "            transformed_data = cp.asnumpy(transformed_data)  # Convert back to NumPy for plotting\n",
    "\n",
    "            # Split data into positive and negative classes\n",
    "            pos_data = transformed_data[labels == 1]\n",
    "            neg_data = transformed_data[labels == 0]\n",
    "\n",
    "            # Plot Positive Examples\n",
    "            ax_pos = axes[row_index, 0]\n",
    "            ax_pos.scatter(pos_data[:, 0], pos_data[:, 1], label=\"Positive\", alpha=0.7, color=\"navy\", s=12)\n",
    "            ax_pos.set_title(f\"Positive - Perp={perplexity}, Exag={early_exaggeration}\")\n",
    "\n",
    "\n",
    "            # Plot Negative Examples\n",
    "            ax_neg = axes[row_index, 1]\n",
    "            ax_neg.scatter(neg_data[:, 0], neg_data[:, 1], label=\"Negative\", alpha=0.7, color=\"pink\", s=12)\n",
    "            ax_neg.set_title(f\"Negative - Perp={perplexity}, Exag={early_exaggeration}\")\n",
    "\n",
    "\n",
    "            # Plot Combined Examples\n",
    "            ax_comb = axes[row_index, 2]\n",
    "            ax_comb.scatter(neg_data[:, 0], neg_data[:, 1], label=\"Negative\", alpha=0.7, color=\"pink\", s=12)\n",
    "            ax_comb.scatter(pos_data[:, 0], pos_data[:, 1], label=\"Positive\", alpha=0.7, color=\"navy\", s=12)\n",
    "            ax_comb.set_title(f\"Combined - Perp={perplexity}, Exag={early_exaggeration}\")\n",
    "\n",
    "\n",
    "            row_index += 1\n",
    "\n",
    "    # Add a single shared legend for all subplots\n",
    "    handles, labels = ax_comb.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper right')\n",
    "\n",
    "    plt.suptitle(title, fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot to the output directory\n",
    "    save_path = os.path.join(output_dir, f\"{title.replace(' ', '_')}.png\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8URwDf5Hf0a"
   },
   "source": [
    "**The perplexity parameter controls how many neighbors each point considers when computing similarity, which defines the balance between local and global structure in the visualization.*\n",
    "\n",
    "\n",
    "*   Normally Perplexity should be ≈ dataset size / 3.\n",
    "*   Low (5-10)\tFocuses on local clusters but may miss global structure.\n",
    "*   Medium (20-50)\tGood balance between local and global structure.\n",
    "*   High (50-100)\tPreserves global structure, but local clusters may merge incorrectly.\n",
    "\n",
    "\n",
    "\n",
    "**The early_exaggeration controls how widely separated clusters are at the beginning of optimization. It helps t-SNE spread out the data before refining the structure.*\n",
    "\n",
    "*   Default early_exaggeration=12\n",
    "*   Low (1-10)\tClusters form slowly, potentially overlapping.\n",
    "*   Medium (12-24)\tGood balance, allowing structure to spread initially.\n",
    "*   High (25-50)\tOverseparates clusters, making structure less natural.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "No2jbkecC7SV",
    "outputId": "e2c8e42a-a683-48bb-dda6-7a702bec352f"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# k values falready defined in step3: k_values = [4, 5, 6]\n",
    "perplexities = [5, 10, 20]  # Adjusted for large datasets\n",
    "early_exaggerations = [12, 24, 48]  # Different exaggeration values to test\n",
    "\n",
    "# Perform GPU-accelerated t-SNE visualization for different k values\n",
    "for k in k_values:\n",
    "    print(f\"\\n### t-SNE Grid Visualization for k={k} ###\")\n",
    "\n",
    "    # Load encoded datasets from encoded_experiments\n",
    "    encoded_exp1, labels_exp1 = encoded_experiments[k][\"experiment_1\"]\n",
    "    encoded_exp2, labels_exp2 = encoded_experiments[k][\"experiment_2\"]\n",
    "\n",
    "    # Generate grid view for Experiment 1\n",
    "    plot_tsne_grid(encoded_exp1, labels_exp1, title=f\"t-SNE Grid (Experiment 1, k={k})\",\n",
    "                   perplexities=perplexities, exaggerations=early_exaggerations)\n",
    "\n",
    "    # Generate grid view for Experiment 2\n",
    "    plot_tsne_grid(encoded_exp2, labels_exp2, title=f\"t-SNE Grid (Experiment 2, k={k})\",\n",
    "                   perplexities=perplexities, exaggerations=early_exaggerations)\n",
    "\n",
    "# Zip the saved plots\n",
    "zip_path = \"tsne_plots.zip\"\n",
    "shutil.make_archive(\"tsne_plots\", 'zip', output_dir)\n",
    "print(f\"All plots have been saved and zipped to {zip_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xjb5fz9rQHJ9"
   },
   "source": [
    "#5. Data Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqProefCqcHP"
   },
   "source": [
    "Divide dataset into distinct training and testing subsets, allocating 20% of the data specifically for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "isHgsj6S3-KP"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to partition dataset into training and testing sets\n",
    "def partition_data(encoded_data, labels, test_size=0.2, random_state=42):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(encoded_data, labels, test_size=test_size, random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhC1EfBS4FWX"
   },
   "source": [
    "Create partitioned datasets for each k-mer encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "0s6o9DtSQHej"
   },
   "outputs": [],
   "source": [
    "\n",
    "partitioned_experiments = {\n",
    "    k: {\n",
    "        \"experiment_1\": partition_data(*encoded_experiments[k][\"experiment_1\"]),\n",
    "        \"experiment_2\": partition_data(*encoded_experiments[k][\"experiment_2\"]),\n",
    "    }\n",
    "    for k in k_values\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iD-GSqu3Qjsd"
   },
   "source": [
    "#6. Training and Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BkNnMQqqnqa"
   },
   "source": [
    "For each experimental condition:\n",
    "\n",
    "*   Train a logistic regression.\n",
    "*   Measure the performance of your model on the test set. Use the method classification_report to show the precision, recall, and f1-score.\n",
    "*   Show the resulting confusion matrix.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBB0dDMD4KeR"
   },
   "source": [
    "Import library for LogisticRegression and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "CiI7kIVH4Kme"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Iz4pG-W4QhN"
   },
   "source": [
    "Trains a logistic regression model and evaluates its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "4-T8KqNq4RWM"
   },
   "outputs": [],
   "source": [
    "# Function to train and evaluate logistic regression\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test, title):\n",
    "\n",
    "    # Train Logistic Regression model\n",
    "    clf = LogisticRegression(max_iter=500, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Print classification report\n",
    "    print(f\"\\n### Classification Report for {title} ###\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"])\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(f\"Confusion Matrix for {title}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJKlY-dh4YGc"
   },
   "source": [
    "Train and test model for each experimental condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "qTTDv2ozQj1N",
    "outputId": "533e3de1-ada6-4fb4-9444-ecf0913fc21e"
   },
   "outputs": [],
   "source": [
    "\n",
    "for k in k_values:\n",
    "    print(f\"\\n### Training Logistic Regression for k={k} ###\")\n",
    "\n",
    "    # Load partitioned datasets\n",
    "    (X_train_exp1, X_test_exp1, y_train_exp1, y_test_exp1) = partitioned_experiments[k][\"experiment_1\"]\n",
    "    (X_train_exp2, X_test_exp2, y_train_exp2, y_test_exp2) = partitioned_experiments[k][\"experiment_2\"]\n",
    "\n",
    "    # Train and evaluate for Experiment 1\n",
    "    train_and_evaluate(X_train_exp1, X_test_exp1, y_train_exp1, y_test_exp1, title=f\"Experiment 1 (k={k})\")\n",
    "\n",
    "    # Train and evaluate for Experiment 2\n",
    "    train_and_evaluate(X_train_exp2, X_test_exp2, y_train_exp2, y_test_exp2, title=f\"Experiment 2 (k={k})\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
